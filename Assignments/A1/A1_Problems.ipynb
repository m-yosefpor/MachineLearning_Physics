{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "A1.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "metadata": {
        "colab_type": "code",
        "id": "rZMH8IvuNf_s",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "i2p4g7reNpTc",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "'''Set the defaults for your plots.'''\n",
        "# plt.rcParams.update({'font.size': 20, 'figsize':(8,6)})\n",
        "SMALL_SIZE = 12\n",
        "MEDIUM_SIZE = 15\n",
        "BIGGER_SIZE = 18\n",
        "\n",
        "plt.rc('font', size=BIGGER_SIZE)          # controls default text sizes\n",
        "plt.rc('axes', titlesize=MEDIUM_SIZE)     # fontsize of the axes title\n",
        "plt.rc('axes', labelsize=BIGGER_SIZE)    # fontsize of the x and y labels\n",
        "plt.rc('xtick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
        "plt.rc('ytick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
        "plt.rc('legend', fontsize=MEDIUM_SIZE)    # legend fontsize\n",
        "plt.rc('figure', titlesize=BIGGER_SIZE)  # fontsize of the figure title\n",
        "\n",
        "plt.rc('axes', linewidth = 3 )\n",
        "plt.rc('lines', linewidth = 3 )\n",
        "\n",
        "plt.rc('lines', markersize = 5 )\n",
        "\n",
        "plt.rc('figure', figsize=(8,4) )         # Image size\n",
        "\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "W0Iyp82aPx3e"
      },
      "cell_type": "markdown",
      "source": [
        "## Non-convex loss function"
      ]
    },
    {
      "metadata": {
        "id": "BE0g3lsFd0iK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "'''\n",
        "1.A: Implement the l2-loss function for a classification problem, \n",
        "i.e. the inputs are y in {0,1} and you are given a list of data in the \n",
        "shape of (n_sample, n_features), the w_list for the linear fit to the \n",
        "decision boundary and the corresponding bias. \n",
        "Fill in the function. \n",
        "\n",
        "'''\n",
        "\n",
        "def l2_loss(y_list, x_list, w_list, bias=0):\n",
        "                  \n",
        "    n_sample = len(y_list)\n",
        "    y_list = y_list.reshape(n_sample,1)\n",
        "\n",
        "    '''\n",
        "    Define z, a\n",
        "    Calculate loss\n",
        "    '''\n",
        "#     z_list = \n",
        "#     a_list = \n",
        "\n",
        "#     loss = \n",
        "    loss = 0\n",
        "    return loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NK6F5jENd0ig",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "'''\n",
        "1.B: Repeat section A for the cross-entropy loss functions.\n",
        "You need to first implement the sigmoid function and the cost for a single sample. \n",
        "Then make the main loss function. \n",
        "'''\n",
        "\n",
        "def sigmoid(x):\n",
        "    \n",
        "    return 0  ##> Change this to what it should be\n",
        "\n",
        "def cross_entropy(y, a):\n",
        "    return 0 ##> Change this to what it should be\n",
        "\n",
        "def cross_entropy_loss(y_list, x_list, w_list, bias=0):\n",
        "                  \n",
        "    ''' \n",
        "    First, you need to calculate the a = sig(z).\n",
        "    Then calculate the loss using the equation for cross entropy. \n",
        "    \n",
        "    '''\n",
        "    n_sample = len(y_list)\n",
        "    y_list = y_list.reshape(n_sample,1)\n",
        "    \n",
        "    '''\n",
        "    Define z, a\n",
        "    Calculate loss\n",
        "    '''\n",
        "#     z_list = \n",
        "#     a_list = \n",
        "\n",
        "#     loss = \n",
        "    loss = 0\n",
        "    return loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SbEyILmKd0is",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "'''\n",
        "1.A: Implement a function that calculates the cross-entropy and hinge loss.\n",
        "First, make a function that calculates the cross-entropy for a single instance. \n",
        "Then use it in the main loss function. \n",
        "\n",
        "'''\n",
        "def hinge_loss(y_list, x_list, w_list, bias=0):\n",
        "                  \n",
        "    ''' \n",
        "    First, you need to calculate the a = sig(z).\n",
        "    Then calculate the loss using the equation for cross entropy. \n",
        "    \n",
        "    '''\n",
        "    n_sample = len(y_list)\n",
        "    y_list = y_list.reshape(n_sample,1)\n",
        "    \n",
        "    '''\n",
        "    Define z, a\n",
        "    Calculate loss\n",
        "    '''\n",
        "#     z_list = \n",
        "#     a_list = \n",
        "\n",
        "#     loss = \n",
        "    loss = 0\n",
        "    return loss\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "O3VOpIHad0i1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Test"
      ]
    },
    {
      "metadata": {
        "id": "jP5FL-E1d0i4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "'''\n",
        "Let's test the loss function over the following data. \n",
        "You need the supplementary file: A1_1.npz. Make sure you\n",
        "took it from the assignment folder. \n",
        "'''\n",
        "\n",
        "\n",
        "data = np.load(\"A1_1.npz\")\n",
        "x1 = data['x1']\n",
        "x2 = data['x2']\n",
        "\n",
        "X = np.array( [x1.tolist(), x2.tolist()] ).T\n",
        "Y = data['y']\n",
        "\n",
        "fig = plt.figure(figsize=(8,6))\n",
        "ax = fig.add_subplot(111)\n",
        "ax.scatter(x1, x2, c=Y, s=30, cmap=plt.cm.flag)\n",
        "plt.show"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iO3J1YD2d0jH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "'''\n",
        "Let's test the loss function over the data. \n",
        "Please note that these losses are not on the same scale and\n",
        "you may need to zoom in or plot them one by one to see the \n",
        "trend clearly. \n",
        "\n",
        "'''\n",
        "\n",
        "##Let's try different lines with differnt slopes. \n",
        "## We make w_list for that. \n",
        "w_list = [ [-i,1] for i in np.arange(-5,5,.1) ]\n",
        "\n",
        "#Now let'calculate the loss for all of these lines (ws)\n",
        "ce_loss_list = [cross_entropy_loss(Y, X, w, b) for w in w_list]\n",
        "l2_loss_list = [l2_loss(Y, X, w, b) for w in w_list]\n",
        "hinge_loss_list = [hinge_loss(Y, X, w, b) for w in w_list]\n",
        "\n",
        "## And plot them!\n",
        "## You may want to change this to plot them seperately. \n",
        "x_label = 'w_1'\n",
        "y_label = 'Loss'\n",
        "plt_title = 'Different loss functions vs the slope, w_1 \\n With W_2 = 1'\n",
        "\n",
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(111, xlabel=x_label, ylabel=y_label, title=plt_title)\n",
        "\n",
        "ax.plot(np.array( w_list )[:,0], l2_loss_list , label='L2')\n",
        "ax.plot(np.array( w_list )[:,0], ce_loss_list , label='Cross Entropy')\n",
        "ax.plot(np.array( w_list )[:,0], hinge_loss_list , label='Hinge')\n",
        "\n",
        "ax.legend(loc=0)\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IQ5iNq4od0jR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Regularization\n",
        "Now, create a regularization function, add it to the loss functions you defined above and check how/it that would change the convexity. "
      ]
    },
    {
      "metadata": {
        "id": "qrzHHbCed0ja",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "'''\n",
        "The regularization term only depends on the w_list (optionally on bias). \n",
        "Let's take p as one input for determining the L^p norm of w. \n",
        "Make the regularization function for the  L^p norm. \n",
        "Optional: Try some other functions. \n",
        "'''\n",
        "\n",
        "def regularization(w_list, p):\n",
        "    'Fill in the regularization!'\n",
        "    return 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vkIpPxwld0js",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "'''\n",
        "Similar to the plot above, plot the loss function with different values of p for L^p norm. \n",
        "'''\n",
        "\n",
        "w_list = [ [-i,1] for i in np.arange(-5,5,.1) ]\n",
        "order= 1  ## This should set the p of the regulariztion norm. \n",
        "\n",
        "ce_loss_list = [cross_entropy_loss(Y, X, w, b)+regularization(w,order) for w in w_list]\n",
        "l2_loss_list = [l2_loss(Y, X, w, b)+regularization(w,order) for w in w_list]\n",
        "hinge_loss_list = [hinge_loss(Y, X, w, b)+regularization(w,order) for w in w_list]\n",
        "\n",
        "x_label = 'w_1'\n",
        "y_label = 'Loss'\n",
        "plt_title = 'Different loss functions vs the slope, w_1 \\n With W_2 = 1 and \\n regularization with l{} norm'.format(order)\n",
        "\n",
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(111, xlabel=x_label, ylabel=y_label, title=plt_title)\n",
        "\n",
        "ax.plot(np.array( w_list )[:,0], l2_loss_list , label='L2')\n",
        "ax.plot(np.array( w_list )[:,0], ce_loss_list , label='Cross Entropy')\n",
        "ax.plot(np.array( w_list )[:,0], hinge_loss_list , label='Hinge')\n",
        "\n",
        "ax.legend(loc=0)\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "87kF4zUDPsqP"
      },
      "cell_type": "markdown",
      "source": [
        "## Optimal poynomial degree"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "ek5Ow8fnNpvl",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "'''\n",
        "Problem 3. \n",
        "Load the black body radiation!\n",
        "\n",
        "- Train (using only the training data) linear and Ridge classifiers for different values of p in [1, 2,... 11]\n",
        "- Plot some of them for comparison\n",
        "- Plot the score of the regressor for the test set vs p, the degree of polynomial. \n",
        "- Specify the optimal p for both ridge and linearRegression. \n",
        "- Optional: Play with SVR and see if you can get a good fit. \n",
        "\n",
        "'''\n",
        "\n",
        "bbr_data_url = \"https://github.com/sraeisi/MachineLearning_Physics/blob/master/Lec_1/bbr.xls?raw=true\"\n",
        "\n",
        "bbr_data = pd.read_excel(bbr_data_url)\n",
        "\n",
        "# bbr_data = pd.read_excel('bbr.xls')\n",
        "bbr_data.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "-SW6swJMNvSm",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "### This forms the data we are going to use. \n",
        "X = bbr_data[0].values.reshape(len(bbr_data),1)\n",
        "Y = 10**17*bbr_data['T'].values#.reshape(len(bbr_data),1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "oYF4eLIjOChx",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "## You need the following packages\n",
        "from sklearn.preprocessing import PolynomialFeatures  ## This is try polynomial fits. \n",
        "from sklearn.model_selection import train_test_split  ## This is to make a training and test set\n",
        "from sklearn.linear_model import LinearRegression, Ridge  ## These are the two classifiers you should try for this problem. "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "kFW4m3liOeOo",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "## This makes your training and test data\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X,Y, random_state=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "mXKPKk3FN6vm",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "'''\n",
        "For p in [1,2,..11], train the regressors, \n",
        "plot the fits, \n",
        "\n",
        "\n",
        "'''\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MA8u_DHXd0lH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "'''\n",
        "claculate the score for all values of p on the test data\n",
        "and plot the score for both Ridge and LinearRegression \n",
        "and put them into \n",
        "lin_scores\n",
        "ridg_scores\n",
        "'''\n",
        "\n",
        "x_label = 'Polynomial Degree, p'\n",
        "y_label = 'Score of the Regressor'\n",
        "plt_title = 'Performance of the regressor vs. p'\n",
        "\n",
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(111, xlabel=x_label, ylabel=y_label, title=plt_title)\n",
        "\n",
        "\n",
        "ax.plot(range(1,12) , lin_scores, label = 'Simple Reg.')\n",
        "ax.plot(range(1,12) , ridg_scores, label = 'Ridge Reg.')\n",
        "\n",
        "ax.legend(loc=0)\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "l6EsgWKEP4Om"
      },
      "cell_type": "markdown",
      "source": [
        "## Which features are more important"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "PBg5RGAzP_XG"
      },
      "cell_type": "markdown",
      "source": [
        "Use the data provided for the ground state energy, train a linear regressor and determine the more important features. \n",
        "- Keep the top 5 and plot the corresponding features vs the ground state energy for the data in the dataset. \n",
        "- Use the tree classifier that we traind and call the important_features. Do they agree with the amplitudes of the linear classifer (from .coefs_). \n",
        "- Use only these five features for training. How does the score change. \n",
        "- Increase the number of features you are keeping and plot the score vs #features. \n",
        "\n",
        "You can find the data here\n",
        "\n",
        "https://www.dropbox.com/s/das9t6vwp7t4b7f/roboBohr.csv.zip?dl=1\n"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "etopFB71P3X-",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}